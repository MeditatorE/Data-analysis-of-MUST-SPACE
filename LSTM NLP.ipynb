{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "94f53228",
   "metadata": {},
   "source": [
    "# LSTM NLP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a711a912",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=UserWarning, module='gensim')\n",
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "import jieba\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.compat.v1.disable_eager_execution()\n",
    "import numpy as np\n",
    "import time\n",
    "from random import randint\n",
    "from random import shuffle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fbc8a011",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea80951e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#停用词获取\n",
    "def makeStopWord():\n",
    "    with open('stopword.txt','r',encoding = 'utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    stopWord = []\n",
    "    for line in lines:\n",
    "        words = jieba.lcut(line,cut_all = False)\n",
    "        for word in words:\n",
    "            stopWord.append(word)\n",
    "    return stopWord\n",
    "\n",
    "\n",
    "\n",
    "#获得文件中的数据，并且分词，去除其中的停用词\n",
    "def getWords(file):\n",
    "    wordList = []\n",
    "    trans = []\n",
    "    lineList = []\n",
    "    with open(file,'r',encoding='utf-8') as f:\n",
    "        lines = f.readlines()\n",
    "    for line in lines:\n",
    "        trans = jieba.lcut(line.replace('\\n',''), cut_all = False)\n",
    "        for word in trans:\n",
    "            if word not in stopWord:\n",
    "                wordList.append(word)\n",
    "        lineList.append(wordList)\n",
    "        wordList = []\n",
    "    return lineList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "642dfa8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将词转化为数组\n",
    "def words2Array(lineList):\n",
    "    linesArray=[]\n",
    "    wordsArray=[]\n",
    "    steps = []\n",
    "    for line in lineList:\n",
    "        t = 0\n",
    "        p = 0\n",
    "        for i in range(MAX_SIZE):\n",
    "            if i<len(line):\n",
    "                try:\n",
    "                    wordsArray.append(model.wv.word_vec(line[i]))\n",
    "                    p = p + 1\n",
    "                except KeyError:\n",
    "                    t=t+1\n",
    "                    continue\n",
    "            else:\n",
    "               wordsArray.append(np.array([0.0]*dimsh))\n",
    "        for i in range(t):\n",
    "            wordsArray.append(np.array([0.0]*dimsh))\n",
    "        steps.append(p)\n",
    "        linesArray.append(wordsArray)\n",
    "        wordsArray = []\n",
    "    linesArray = np.array(linesArray)\n",
    "    steps = np.array(steps)\n",
    "    return linesArray, steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5747befc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#将数据转化为数据\n",
    "def convert2Data(posArray, negArray, posStep, negStep):\n",
    "    randIt = []\n",
    "    data = []\n",
    "    steps = []\n",
    "    labels = []\n",
    "    for i in range(len(posArray)):\n",
    "        randIt.append([posArray[i], posStep[i], [1,0]])\n",
    "    for i in range(len(negArray)):\n",
    "        randIt.append([negArray[i], negStep[i], [0,1]])\n",
    "    shuffle(randIt)\n",
    "    for i in range(len(randIt)):\n",
    "        data.append(randIt[i][0])\n",
    "        steps.append(randIt[i][1])\n",
    "        labels.append(randIt[i][2])\n",
    "    data = np.array(data)\n",
    "    steps = np.array(steps)\n",
    "    return data, steps, labels\n",
    "\n",
    "\n",
    "\n",
    "#产生训练数据集和测试数据集\n",
    "def makeData(posPath,negPath):\n",
    "    #获取词汇，返回类型为[[word1,word2...],[word1,word2...],...]\n",
    "    pos = getWords(posPath)\n",
    "    print(\"The positive data's length is :\",len(pos))\n",
    "    neg = getWords(negPath)\n",
    "    print(\"The negative data's length is :\",len(neg))\n",
    "    #将评价数据转换为矩阵，返回类型为array\n",
    "    posArray, posSteps = words2Array(pos)\n",
    "    negArray, negSteps = words2Array(neg)\n",
    "    #将积极数据和消极数据混合在一起打乱，制作数据集\n",
    "    Data, Steps, Labels = convert2Data(posArray, negArray, posSteps, negSteps)\n",
    "    return Data, Steps, Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "529e0c70",
   "metadata": {},
   "outputs": [],
   "source": [
    "#产生用于分类的数据集\n",
    "def makeTestData(dataPath):\n",
    "    #获取词汇，返回类型为[[word1,word2...],[word1,word2...],...]\n",
    "    data = getWords(dataPath)\n",
    "    print(\"The data's length is :\",len(data))\n",
    "    #将评价数据转换为矩阵，返回类型为array\n",
    "    dataArray, dataSteps = words2Array(data)\n",
    "    #将积极数据和消极数据混合在一起打乱，制作数据集\n",
    "    Data, Steps = testConvert2Data(dataArray, dataSteps)\n",
    "    return Data, Steps\n",
    "\n",
    "\n",
    "\n",
    "def testConvert2Data(dataArray, dataStep):\n",
    "    randIt = []\n",
    "    data = []\n",
    "    steps = []\n",
    "    labels = []\n",
    "    for i in range(len(dataArray)):\n",
    "        randIt.append([dataArray[i], dataStep[i]])\n",
    "    shuffle(randIt)\n",
    "    for i in range(len(randIt)):\n",
    "        data.append(randIt[i][0])\n",
    "        steps.append(randIt[i][1])\n",
    "    data = np.array(data)\n",
    "    steps = np.array(steps)\n",
    "    return data, steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfe105fc",
   "metadata": {},
   "source": [
    "# 设置停用词并创建词向量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "087c2909",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /var/folders/fw/lzdssnb90018371gm3r0tgph0000gn/T/jieba.cache\n",
      "Loading model cost 0.521 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    }
   ],
   "source": [
    "timeA=time.time()\n",
    "word2vec_path = './word2vec/200/word2vec.model'\n",
    "model=gensim.models.Word2Vec.load(word2vec_path)\n",
    "dimsh=model.vector_size\n",
    "MAX_SIZE=25\n",
    "stopWord = makeStopWord()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39957ba5",
   "metadata": {},
   "source": [
    "# 统计评论数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d63dd602",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In train data:\n",
      "The positive data's length is : 7840\n",
      "The negative data's length is : 10853\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-3-b8948e57150c>:12: DeprecationWarning: Call to deprecated `word_vec` (Use get_vector instead).\n",
      "  wordsArray.append(model.wv.word_vec(line[i]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "In test data:\n",
      "The positive data's length is : 1961\n",
      "The negative data's length is : 2714\n",
      "In real data:\n",
      "The data's length is : 39199\n"
     ]
    }
   ],
   "source": [
    "print(\"In train data:\")\n",
    "trainData, trainSteps, trainLabels = makeData('data/train/Pos-train.txt',\n",
    "                                              'data/train/Neg-train.txt')\n",
    "print(\"In test data:\")\n",
    "testData, testSteps, testLabels = makeData('data/test/Pos-test.txt', 'data/test/Neg-test.txt')\n",
    "\n",
    "print(\"In real data:\")\n",
    "realData, realSteps = makeTestData('data/real/all_posts.txt')\n",
    "\n",
    "trainLabels = np.array(trainLabels)\n",
    "\n",
    "del model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48a88e13",
   "metadata": {},
   "source": [
    "# 模型准备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "4b149624",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_nodes = 128\n",
    "batch_size = 16\n",
    "output_size = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "75a5f226",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(dataset, steps):\n",
    "    outputs, last_states = tf.nn.dynamic_rnn(cell = lstm_cell, dtype = tf.float32, sequence_length = steps, inputs = dataset)\n",
    "    hidden = last_states[-1]\n",
    "\n",
    "    hidden = tf.matmul(hidden, w1) + b1\n",
    "    logits = tf.matmul(hidden, w2) + b2\n",
    "    return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7c815847",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-929b241e107e>:2: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
      "WARNING:tensorflow:From /Users/einstean/opt/anaconda3/lib/python3.8/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:750: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /Users/einstean/opt/anaconda3/lib/python3.8/site-packages/tensorflow/python/util/dispatch.py:206: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See `tf.nn.softmax_cross_entropy_with_logits_v2`.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/einstean/opt/anaconda3/lib/python3.8/site-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:699: UserWarning: `tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
      "  warnings.warn(\"`tf.nn.rnn_cell.BasicLSTMCell` is deprecated and will be \"\n",
      "/Users/einstean/opt/anaconda3/lib/python3.8/site-packages/keras/engine/base_layer_v1.py:1684: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
      "  warnings.warn('`layer.add_variable` is deprecated and '\n"
     ]
    }
   ],
   "source": [
    "graph = tf.Graph()\n",
    "\n",
    "with graph.as_default():\n",
    "\n",
    "    tf_train_dataset = tf.placeholder(tf.float32,shape=(batch_size,MAX_SIZE,dimsh))\n",
    "    tf_train_steps = tf.placeholder(tf.int32,shape=(batch_size))\n",
    "    tf_train_labels = tf.placeholder(tf.float32,shape=(batch_size,output_size))\n",
    "\n",
    "    tf_test_dataset = tf.constant(testData,tf.float32)\n",
    "    tf_test_steps = tf.constant(testSteps,tf.int32)\n",
    "\n",
    "    tf_real_dataset = tf.constant(realData,tf.float32)\n",
    "    tf_real_steps = tf.constant(realSteps,tf.int32)\n",
    "\n",
    "    lstm_cell = tf.nn.rnn_cell.BasicLSTMCell(num_units = num_nodes,\n",
    "                                             state_is_tuple=True)\n",
    "\n",
    "    w1 = tf.Variable(tf.truncated_normal([num_nodes,num_nodes // 2], stddev=0.1))\n",
    "    b1 = tf.Variable(tf.truncated_normal([num_nodes // 2], stddev=0.1))\n",
    "\n",
    "    w2 = tf.Variable(tf.truncated_normal([num_nodes // 2, 2], stddev=0.1))\n",
    "    b2 = tf.Variable(tf.truncated_normal([2], stddev=0.1))\n",
    "    \n",
    "    #-----------------------------------训练及测试模型-------------------------------------   \n",
    "    # 训练模型\n",
    "    train_logits = model(tf_train_dataset, tf_train_steps)\n",
    "    # 损失函数\n",
    "    loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=train_logits))\n",
    "    # 优化器\n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.01).minimize(loss)\n",
    "    # 测试模型\n",
    "    test_prediction = tf.nn.softmax(model(tf_test_dataset, tf_test_steps))\n",
    "    # 分类\n",
    "    real_prediction = tf.nn.softmax(model(tf_real_dataset, tf_real_steps))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c1ff867",
   "metadata": {},
   "source": [
    "# 训练/测试模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "93126e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_steps = 20001\n",
    "summary_frequency = 500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ae8b616d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "The step is: 500\n",
      "In train data,the loss is:0.6055\n",
      "In test data,the accuracy is:72.90%\n",
      "The step is: 1000\n",
      "In train data,the loss is:0.5389\n",
      "In test data,the accuracy is:74.65%\n",
      "The step is: 1500\n",
      "In train data,the loss is:0.5084\n",
      "In test data,the accuracy is:75.66%\n",
      "The step is: 2000\n",
      "In train data,the loss is:0.4923\n",
      "In test data,the accuracy is:76.43%\n",
      "The step is: 2500\n",
      "In train data,the loss is:0.4747\n",
      "In test data,the accuracy is:77.13%\n",
      "The step is: 3000\n",
      "In train data,the loss is:0.4655\n",
      "In test data,the accuracy is:77.01%\n",
      "The step is: 3500\n",
      "In train data,the loss is:0.4511\n",
      "In test data,the accuracy is:77.16%\n",
      "The step is: 4000\n",
      "In train data,the loss is:0.4406\n",
      "In test data,the accuracy is:77.78%\n",
      "The step is: 4500\n",
      "In train data,the loss is:0.4350\n",
      "In test data,the accuracy is:77.50%\n",
      "The step is: 5000\n",
      "In train data,the loss is:0.4211\n",
      "In test data,the accuracy is:78.03%\n",
      "The step is: 5500\n",
      "In train data,the loss is:0.4094\n",
      "In test data,the accuracy is:78.42%\n",
      "The step is: 6000\n",
      "In train data,the loss is:0.4039\n",
      "In test data,the accuracy is:78.65%\n",
      "The step is: 6500\n",
      "In train data,the loss is:0.3873\n",
      "In test data,the accuracy is:76.83%\n",
      "The step is: 7000\n",
      "In train data,the loss is:0.3801\n",
      "In test data,the accuracy is:78.63%\n",
      "The step is: 7500\n",
      "In train data,the loss is:0.3683\n",
      "In test data,the accuracy is:78.20%\n",
      "The step is: 8000\n",
      "In train data,the loss is:0.3584\n",
      "In test data,the accuracy is:78.87%\n",
      "The step is: 8500\n",
      "In train data,the loss is:0.3495\n",
      "In test data,the accuracy is:78.44%\n",
      "The step is: 9000\n",
      "In train data,the loss is:0.3306\n",
      "In test data,the accuracy is:77.18%\n",
      "The step is: 9500\n",
      "In train data,the loss is:0.3290\n",
      "In test data,the accuracy is:78.37%\n",
      "The step is: 10000\n",
      "In train data,the loss is:0.3051\n",
      "In test data,the accuracy is:77.13%\n",
      "The step is: 10500\n",
      "In train data,the loss is:0.2984\n",
      "In test data,the accuracy is:78.37%\n",
      "The step is: 11000\n",
      "In train data,the loss is:0.2847\n",
      "In test data,the accuracy is:76.58%\n",
      "The step is: 11500\n",
      "In train data,the loss is:0.2687\n",
      "In test data,the accuracy is:77.73%\n",
      "The step is: 12000\n",
      "In train data,the loss is:0.2612\n",
      "In test data,the accuracy is:77.99%\n",
      "The step is: 12500\n",
      "In train data,the loss is:0.2365\n",
      "In test data,the accuracy is:77.60%\n",
      "The step is: 13000\n",
      "In train data,the loss is:0.2366\n",
      "In test data,the accuracy is:77.80%\n",
      "The step is: 13500\n",
      "In train data,the loss is:0.2104\n",
      "In test data,the accuracy is:76.06%\n",
      "The step is: 14000\n",
      "In train data,the loss is:0.1991\n",
      "In test data,the accuracy is:77.80%\n",
      "The step is: 14500\n",
      "In train data,the loss is:0.1890\n",
      "In test data,the accuracy is:77.63%\n",
      "The step is: 15000\n",
      "In train data,the loss is:0.1634\n",
      "In test data,the accuracy is:77.30%\n",
      "The step is: 15500\n",
      "In train data,the loss is:0.1604\n",
      "In test data,the accuracy is:77.28%\n",
      "The step is: 16000\n",
      "In train data,the loss is:0.1393\n",
      "In test data,the accuracy is:76.79%\n",
      "The step is: 16500\n",
      "In train data,the loss is:0.1349\n",
      "In test data,the accuracy is:76.90%\n",
      "The step is: 17000\n",
      "In train data,the loss is:0.1161\n",
      "In test data,the accuracy is:76.60%\n",
      "The step is: 17500\n",
      "In train data,the loss is:0.1052\n",
      "In test data,the accuracy is:77.78%\n",
      "The step is: 18000\n",
      "In train data,the loss is:0.1022\n",
      "In test data,the accuracy is:77.11%\n",
      "The step is: 18500\n",
      "In train data,the loss is:0.0827\n",
      "In test data,the accuracy is:77.60%\n",
      "The step is: 19000\n",
      "In train data,the loss is:0.0826\n",
      "In test data,the accuracy is:77.16%\n",
      "The step is: 19500\n",
      "In train data,the loss is:0.0703\n",
      "In test data,the accuracy is:76.81%\n",
      "The step is: 20000\n",
      "In train data,the loss is:0.0687\n",
      "In test data,the accuracy is:77.20%\n",
      "\n",
      "Start object classification\n",
      "\n",
      "1000  posts have been classified.\n",
      "\n",
      "2000  posts have been classified.\n",
      "\n",
      "3000  posts have been classified.\n",
      "\n",
      "4000  posts have been classified.\n",
      "\n",
      "5000  posts have been classified.\n",
      "\n",
      "6000  posts have been classified.\n",
      "\n",
      "7000  posts have been classified.\n",
      "\n",
      "8000  posts have been classified.\n",
      "\n",
      "9000  posts have been classified.\n",
      "\n",
      "10000  posts have been classified.\n",
      "\n",
      "11000  posts have been classified.\n",
      "\n",
      "12000  posts have been classified.\n",
      "\n",
      "13000  posts have been classified.\n",
      "\n",
      "14000  posts have been classified.\n",
      "\n",
      "15000  posts have been classified.\n",
      "\n",
      "16000  posts have been classified.\n",
      "\n",
      "17000  posts have been classified.\n",
      "\n",
      "18000  posts have been classified.\n",
      "\n",
      "19000  posts have been classified.\n",
      "\n",
      "20000  posts have been classified.\n",
      "\n",
      "21000  posts have been classified.\n",
      "\n",
      "22000  posts have been classified.\n",
      "\n",
      "23000  posts have been classified.\n",
      "\n",
      "24000  posts have been classified.\n",
      "\n",
      "25000  posts have been classified.\n",
      "\n",
      "26000  posts have been classified.\n",
      "\n",
      "27000  posts have been classified.\n",
      "\n",
      "28000  posts have been classified.\n",
      "\n",
      "29000  posts have been classified.\n",
      "\n",
      "30000  posts have been classified.\n",
      "\n",
      "31000  posts have been classified.\n",
      "\n",
      "32000  posts have been classified.\n",
      "\n",
      "33000  posts have been classified.\n",
      "\n",
      "34000  posts have been classified.\n",
      "\n",
      "35000  posts have been classified.\n",
      "\n",
      "36000  posts have been classified.\n",
      "\n",
      "37000  posts have been classified.\n",
      "\n",
      "38000  posts have been classified.\n",
      "\n",
      "39000  posts have been classified.\n",
      "\n",
      "Positive:  16910\n",
      "Negative:  22289\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph = graph) as session:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    mean_loss = 0\n",
    "    for step in range(num_steps):\n",
    "        offset = (step * batch_size) % (len(trainLabels)-batch_size)\n",
    "        feed_dict={tf_train_dataset:trainData[offset:offset + batch_size],\n",
    "                   tf_train_labels:trainLabels[offset:offset + batch_size],\n",
    "                   tf_train_steps:trainSteps[offset:offset + batch_size]}\n",
    "        \n",
    "        # 开始训练模型\n",
    "        _, l = session.run([optimizer,loss],\n",
    "                           feed_dict = feed_dict)\n",
    "        mean_loss += l\n",
    "        if step >0 and step % summary_frequency == 0:\n",
    "            mean_loss = mean_loss / summary_frequency\n",
    "            print(\"The step is: %d\"%(step))\n",
    "            print(\"In train data,the loss is:%.4f\"%(mean_loss))\n",
    "            mean_loss = 0\n",
    "            acrc = 0\n",
    "            pos = 0\n",
    "            neg = 0\n",
    "            # print(test_prediction)\n",
    "            # 开始测试模型\n",
    "            prediction = session.run(test_prediction)\n",
    "            \n",
    "            for i in range(len(prediction)):\n",
    "                if prediction[i][testLabels[i].index(1)] > 0.5:\n",
    "                    # print(prediction[i])\n",
    "                    acrc = acrc + 1\n",
    "            # print(testLabels)\n",
    "            print(\"In test data,the accuracy is:%.2f%%\"%((acrc/len(testLabels))*100))\n",
    "        \n",
    "        #-----------------------------------开始分类-------------------------------------   \n",
    "        if(step==20000):\n",
    "                prediction = session.run(real_prediction)\n",
    "                for i in range(len(prediction)):\n",
    "                    if i == 0:\n",
    "                        print(\"\\nStart object classification\\n\")\n",
    "                    if i%1000 == 0 and i != 0:\n",
    "                        print(i, \" posts have been classified.\\n\")\n",
    "                    if prediction[i][0] > 0.5:\n",
    "                        # print(prediction[i])\n",
    "                        pos += 1\n",
    "                    else:\n",
    "                        neg += 1\n",
    "                print(\"Positive: \",pos)\n",
    "                print(\"Negative: \",neg)\n",
    "                print(\"Done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf53e815",
   "metadata": {},
   "source": [
    "# 算时间"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "49d70105",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time cost: 272\n"
     ]
    }
   ],
   "source": [
    "timeB=time.time()\n",
    "print(\"time cost:\",int(timeB-timeA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "483f201f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
